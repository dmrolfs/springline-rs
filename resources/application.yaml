---
engine:
  machine_id: 1
  node_id: 1
  # Specify the span for telemetry collection into a portfolio available to eligibility and decision
  # policies. It's more transparent if this span is set to a duration above what is used in a policy
  # rule, but it is not required. If a policy rule exceeds this configured span, then coverage of
  # the portfolio must be more than half, otherwise the condition is not met.
  # The default is 10 minutes.
  telemetry_portfolio_window_secs: 600

prometheus:
  # The address the Prometheus server should bind to. Default is "9898".
  port: 9898

http:
  host: 0.0.0.0
  port: 8000

flink:
  job_manager_uri_scheme: http
#  job_manager_host: localhost
  job_manager_port: 8081
  max_retries: 3
  min_retry_interval_millis: 1000
  max_retry_interval_millis: 300000
  pool_idle_timeout_secs: 60
  pool_max_idle_per_host: 5

# configure the kubernetes client used by both sensors and actuators.
kubernetes:
  # The Kubernetes clients supports several different configuration strategies:
  # - `infer`: Infers configuration from the environment. First attempting to load in-cluster
  # environment variables, then if that fails, trying the local kubeconfig. Infer is the common
  # setting.
  # - `local_url`: Constructs client where only the cluster_url is set and everything else is
  # set to default values.
  # - `cluster_env`: Creates configuration from the cluster's environment variables following
  # the standard API Access from a Pod and relies on you having the service account's token
  # mounted, as well as having given the service account rbac access to what you need.
  # - `kube_config`: Create configuration from the default local configu file, respecting the
  # `$KUBECONFIG` evar, but otherwise default to ~/.kube/config. You can also customize what
  # context/cluster/user you want to use here, but it will default to the current-context.
  # - `custom_kube_config`: Create configuration from a Kubeconfig struct. This bypasses the n
  # normal config parsing to obtain custom functionality.
  client: infer

  # Name of the kubernetes namespace in which to work. If not set, the default
  # kubernetes namespace is used.
#  namespace:

  # Period to allow cluster to stabilize after a patch operation. Defaults to 3 minutes.
#  patch_settle_timeout_secs: 180

sensor:
  clearinghouse:
    # Optional clearinghouse ttl configuration.
    ttl:
      # Optional setting for the time to live for each clearinghouse field. The default is 5 minutes.
      # The clearinghouse is cleared during rescaling, so this value should be set to a duration
      # reflecting a break in connectivity or the job not running.
      default_ttl_secs: 300

      # Optional overrides for specific clearinghouse fields. The key is the field name, and the
      # value is the time to live in seconds.
      #ttl_overrides: {}

      # Optional list of clearinghouse fields that never expire.
      never_expire:
        - all_sinks_healthy
        - cluster.is_rescaling
        - cluster.is_deploying
        - cluster.last_deployment

    # Optional setting for the cache access counter kept for admission and eviction. The default is
    # 1000. A good  value is to set nr_counters to be 10x the number of *unique* fields expected to
    # be kept in the clearinghouse cache when full, not necessarily "cost" related.
    nr_counters: 1000

    # Optional setting for maximum cost of the items stored in the cache. The default is 100,
    # which assumes the normal case where the per-item cost is 1.
    max_cost: 100

    # Optional setting to direct how frequent the cache is checked for eviction. The default is 5
    # seconds. A good setting should consider the frequency of data pushed into the clearinghouse.
    cleanup_interval_millis: 5000

  # The `flink` sensor is more complicated that the modular sensors that can be specified via the
  # `sensor` configuration. The `flink` sensor will query the job manager's REST API for the Job
  # scope, TaskManager scope, TaskManager admin. Then it queries for the active jobs, and for each
  # active jobs, the sensor looks for desired metrics from each task vertex. There is a default set
  # of metrics that are queried for, but the user can specify additional metrics to query for via
  # the `metric_orders` configuration.
  flink:
    metrics_initial_delay_secs: 0
    metrics_interval_secs: 15

    # User can add metrics to be collected from Flink beyond the default set. Each metric is defined
    # as an "order" of the form:
    metric_orders:
#      - Operator:
#          name: "Data Input stream"
#          position: source
#          metric: records-lag-max
#          agg: sum
#          telemetry_path: flow.source_records_lag_max
#          telemetry_type: Integer
#
#      - Operator:
#          name: "Data Input stream"
#          position: source
#          metric: assigned-partitions
#          agg: sum
#          telemetry_path: flow.source_assigned_partitions
#          telemetry_type: Integer
#      - Operator:
#          name: "Data Output stream"
#          position: source
#          metric: records-consumed-rate
#          agg: sum
#          telemetry_path: flow.source_records_consumed_rate
#          telemetry_type: Float
#
      - Job:
          metric: lastCheckpointDuration
          agg: max
          telemetry_path: health.last_checkpoint_duration
          telemetry_type: Integer

  # Sensors may be specified and loaded from this block. Currently, two form are supported:
  # 1. `csv` Sensors loaded at startup for the given `path` file.
  # 2. `rest_api` Sensors specified with `interval` (in seconds), HTTP `method`,`url`,
  #    `headers` (array of (String, String) tuples, and `max_retries` for each iteration.
  sensors: {}

eligibility:
  policies:
    - source: file
      policy:
        path: "./resources/eligibility.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/eligibility_ext.polar"
        is_template: true
  template_data:
    policy_extension: eligibility_ext
    cooling_secs: 300
    stable_secs: 300

decision:
  policies:
    - source: file
      policy:
        path: "./resources/decision.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/decision_basis.polar"
        is_template: true
  template_data:
    # policy template that is pulled into the base decision policy template, which forms the basis
    #of the business criteria to scale job.
    basis: decision_basis

    # Optional threshold representing the maximum ratio the total_lag rate / consumed_records_rate
    # that is healthy. This value represents how much the workload is increasing (positive) or
    # decreasing (negative). This value is used for scale up decisions and not scale down decisions.
    max_healthy_relative_lag_velocity: 3.0

    # Optional threshold representing the maximum total_lag that is healthy. This factor does not
    # consider the workload, so its usefulness may be limited compared to the
    # `max_healthy_relative_lag_velocity`. This could be used to define a cap to `total_lag` as a
    # supplemental rule to the relative lag velocity.
    # max_healthy_lag:

    # Optional threshold on the target task utilization, which is calculated based on the
    # `idle_time_millis_per_sec`. This value is used for scale down decisions and is typically
    # coupled with a check that the total_lag is 0 -- meaning the application is caught up on
    #workload and underutilized.
    min_task_utilization: 0.6

    # Optional threshold that may scale up the application if the CPU load is too high. High
    # resource utilization (CPU, Memory, Network)) may suggest the application is destabilizing,
    #which scaling up may help.
    # max_healthy_cpu_load:

    # Optional threshold that may scale up the application if the heap memory load is too high.
    # High resource utilization (CPU, Memory, Network) may suggest the application is destabilizing,
    # which scaling up may help.
    # max_healthy_heap_memory_load:

    # Optional threshold that may scale up the application if network IO is too high.
    # High resource utilization (CPU, Memory, Network) may suggest the application is destabilizing,
    #which scaling up may help.
    # max_healthy_network_io_utilization:


    # Many of the rules look at the rolling average of the values to reduce the affects of
    # short-term spikes. This optional value sets the common evaluation window for these range
    # metric forms.
    # Valid duration may range from 0 (always single item) to 600 secs (10 minutes).
    evaluate_duration_secs: 120

plan:
  performance_repository:
    storage: memory
  min_cluster_size: 1
  min_scaling_step: 2
  restart_secs: 120
  max_catch_up_secs: 600
  recovery_valid_secs: 300
  window: 20
  spike:
    std_deviation_threshold: 5.0
    influence: 0.75
    length_threshold: 3

governance:
  policy:
    policies:
      - source: file
        policy:
          path: "./resources/governance.polar"
  rules:
    min_cluster_size: 0
    max_cluster_size: 20
    min_scaling_step: 2
    max_scaling_step: 4
    custom: {}

action:
  # Timeout for confirmation of scaling actions. Until this timeout expires, springline will poll
  # the kubernetes API to check if the scaling action has been satisfied.
  action_timeout_secs: 600

  taskmanager:
    # A selector to identify taskmanagers the list of returned objects by the scaling target; e.g.,
    # "component=taskmanager"
    label_selector: "component=taskmanager,release=dr-springline"

    # Resource name of the deployment resource used to deploy taskmanagers; e.g. "statefulset/my-taskmanager".
    deploy_resource: statefulset/dr-springline-tm

    kubernetes_api:
      # Timeout for the kubernetes list/watch call.
      #
      # This limits the duration of the call, regardless of any activity or inactivity.
      # If unset for a call, the default is 290s.
      # We limit this to 295s due to [inherent watch limitations](https://github.com/kubernetes/kubernetes/issues/6513).
#      api_timeout_secs: 290

      # Interval to query kubernetes API for the status of the scaling action.
      polling_interval_secs: 5

  flink:
    # Interval in which Flink asynchronous API status is polled. Defaults to 1 second.
    polling_interval_secs: 1

#    savepoint:
      # Time allowed for a savepoint to complete. Defaults to 10 minutes.
#      operation_timeout_secs: 600

      # Optional directory for the triggered savepoint. If not set, the default will the savepoint
      # directory configured with Flink.
#      directory: "/path/to/flink/savepoints"

    restart:
      # Time allowed restarting and confirm a FLink job. Defaults to 1 minute.
      operation_timeout_secs: 60

      # Optionally specify the restore mode when restarting from a savepoint. Defaults to "no_claim".
      # - "no_claim": Flink will not claim ownership of the snapshot files. However it will make sure
      #               it does not depend on any artifacts from the restored snapshot. In order to do
      #               that, Flink will take the first checkpoint as a full one, which means it might
      #               reupload/duplicate files that are part of the restored checkpoint.
      # - "claim": Flink will take ownership of the given snapshot. It will clean the snapshot once
      #            it is subsumed by newer ones.
      # - "legacy": This is the mode in which older version of Flink worked. It will not claim
      #             ownership of the snapshot and will not delete the files. However, it can directly
      #             depend on the existence of the files of the restored checkpoint. It might not be
      #             safe to delete checkpoints that were restored in legacy mode.
      restore_mode: claim

      # Optional boolean value that specifies whether the job submission should be rejected if the
      # savepoint contains state that cannot be mapped back to the job.
#      allow_non_restartable_jobs: false

      # Optional comma-separated list of program arguments
#      program_args:
#        - "--yourFirstParam=1"
#        -  "--yourOtherCustomArg=something"

# Properties for the springline config sensor to inject values that are known at deployment time.
# Property names and types should directly match corresponding subscription requirements as
# submission.
context_stub:
  all_sinks_healthy: true
