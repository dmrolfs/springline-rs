---
engine:
  machine_id: 1
  node_id: 1
  # Specify the span for telemetry collection into a portfolio available to eligibility and decision
  # policies. It's more transparent if this span is set to a duration above what is used in a policy
  # rule, but it is not required. If a policy rule exceeds this configured span, then coverage of
  # the portfolio must be more than half, otherwise the condition is not met.
  # The default is 10 minutes.
  telemetry_window_secs: 600

  # Optional tuning of the coverage percentage required before metric window thresholds are
  # met. This parameter enables "sufficient" quorum coverage of a threshold window to be tripped.
  # This parameter enables policy rules to act on a sufficient telemetry level and enable policy
  # decisions in the event of flapping telemetry. The default value is 0.8, which means at least
  # only 80% of the telemetry data within the window needs to exceed the threshold before the
  # rule is triggered. Values MUST be with (0.0, 1.0] and the default percentile is 0.8.
  telemetry_window_quorum_percentile: 0.8

prometheus:
  # The address the Prometheus server should bind to. Default is "9898".
  port: 9898

http:
  host: 0.0.0.0
  port: 8000

flink:
  job_manager_uri_scheme: http
#  job_manager_host: localhost
  job_manager_port: 8081
  max_retries: 3
  min_retry_interval_millis: 1000
  max_retry_interval_millis: 300000
  pool_idle_timeout_secs: 60
  pool_max_idle_per_host: 5

# configure the kubernetes client used by both sensors and actuators.
kubernetes:
  # The Kubernetes clients supports several different configuration strategies:
  # - `infer`: Infers configuration from the environment. First attempting to load in-cluster
  # environment variables, then if that fails, trying the local kubeconfig. Infer is the common
  # setting.
  # - `local_url`: Constructs client where only the cluster_url is set and everything else is
  # set to default values.
  # - 'in_cluster_dns': Load an in-cluster config using the API server at `https://kubernetes.default.svc`.
  # A service account's token must be available in `/var/run/secrets/kubernetes.io/serviceaccount/`.
  # - `in_cluster_env`: Creates configuration from the cluster's environment variables following
  # the standard API Access from a Pod and relies on you having the service account's token
  # mounted, as well as having given the service account rbac access to what you need.
  # - `kube_config`: Create configuration from the default local config file, respecting the
  # `$KUBECONFIG` evar, but otherwise default to ~/.kube/config. You can also customize what
  # context/cluster/user you want to use here, but it will default to the current-context.
  # - `custom_kube_config`: Create configuration from a Kubeconfig struct. This bypasses the n
  # normal config parsing to obtain custom functionality.
  client: infer

  # Name of the kubernetes namespace in which to work. If not set, the default
  # kubernetes namespace is used.
#  namespace:

  # Period to allow cluster to stabilize after a patch operation. Defaults to 3 minutes.
#  patch_settle_timeout_secs: 180

sensor:
  clearinghouse:
    # Optional clearinghouse ttl configuration.
    ttl:
      # Optional setting for the time to live for each clearinghouse field. The default is 5 minutes.
      # The clearinghouse is cleared during rescaling, so this value should be set to a duration
      # reflecting a break in connectivity or the job not running.
      default_ttl_secs: 300

      # Optional overrides for specific clearinghouse fields. The key is the field name, and the
      # value is the time to live in seconds.
      #ttl_overrides: {}

      # Optional list of clearinghouse fields that never expire.
      never_expire:
        - all_sinks_healthy
        - cluster.is_rescaling
        - cluster.is_deploying
        - cluster.last_deployment

    # Optional setting for the cache access counter kept for admission and eviction. The default is
    # 1000. A good  value is to set nr_counters to be 10x the number of *unique* fields expected to
    # be kept in the clearinghouse cache when full, not necessarily "cost" related.
    nr_counters: 1000

    # Optional setting for maximum cost of the items stored in the cache. The default is 100,
    # which assumes the normal case where the per-item cost is 1.
    max_cost: 100

    # Optional setting to direct how frequent the cache is checked for eviction. The default is 5
    # seconds. A good setting should consider the frequency of data pushed into the clearinghouse.
    cleanup_interval_millis: 5000

  # The `flink` sensor is more complicated that the modular sensors that can be specified via the
  # `sensor` configuration. The `flink` sensor will query the job manager's REST API for the Job
  # scope, TaskManager scope, TaskManager admin. Then it queries for the active jobs, and for each
  # active jobs, the sensor looks for desired metrics from each task vertex. There is a default set
  # of metrics that are queried for, but the user can specify additional metrics to query for via
  # the `metric_orders` configuration.
  flink:
    metrics_initial_delay_secs: 0
    metrics_interval_secs: 15

    # User can add metrics to be collected from Flink beyond the default set. Each metric is defined
    # as an "order" of the form found via the [Flink metrics API](https://nightlies.apache.org/flink/flink-docs-release-1.16/docs/ops/metrics/)
    # A list of [Flink Scope]: [Metric Order structure] items, where
    # Flink Scope is one of the supported Flink scopes: `Job`, `TaskManager`, `Task`, `Operator`,
    # and (experimental) `Derivative`.
    # `Job` and `TaskManager` scopes define a simple `MetricSpec`, aggregated over the cluster, of
    # the form:
    #   - metric: name of the metric exposed by Flink. It should match exactly and be sure to watch
    # out for inconsistent conventions used within Flink.
    #   - agg: one of value, max, min, sum, avg. This specifies how Flink aggregates the metric (see
    # Flink documentation. The exception is `value`, which is used to identify single value metrics.
    #   - telemetry_path: Specifies the springline telemetry identifier used in both telemetry
    # subscriptions for both the main data metric catalog and springline planning contexts.
    #   - telemetry_type: One of Boolean, Integer, Float, Text, Seq, or Table.
    # Metrics in these scopes are usually best applied per specific parts of the job's plan; e.g.,
    # Kafka's `records-lag-max` metric should summed for source operators and it is best to isolate
    # the average of non-source `idleTimeMsPerSecond` measurements since source operators have a
    # very different profile.  Both `Task` and `Operator` further qualify the metric order to
    # support a job plan position specification that enables the targeting of metrics at specific
    # positions in the job. It can be one of:
    # `any`, `source`, `not_source`, `sink`, `not_sink`, and `through`.
    # The `Operator` order further supports filtering to specifically named operators. For example,
    # if you have a source operator named, "Data Input stream", you may isolate metrics
    # (e.g., `records-lag-max`). Springline will look to isolate metrics from source operators that
    # *begin* with that name. (Starting with strategy is applied since Flink may fuse operators
    # into a single vertex.) Similarly, `sink` would look for operators ending with the specified
    # name; and, `through` would look for operators that contain the name.
    # The `Derivative` order type supports a basic combination (e.g., product or sum) of two
    # springline telemetry entries. This is highly experimental and subject to change.
    metric_orders:
#      - Operator:
#          name: "Data Input stream"
#          position: source
#          metric: records-lag-max
#          agg: sum
#          telemetry_path: flow.source_records_lag_max
#          telemetry_type: Integer
#
#      - Operator:
#          name: "Data Input stream"
#          position: source
#          metric: assigned-partitions
#          agg: sum
#          telemetry_path: flow.source_assigned_partitions
#          telemetry_type: Integer
#      - Operator:
#          name: "Data Output stream"
#          position: source
#          metric: records-consumed-rate
#          agg: sum
#          telemetry_path: flow.source_records_consumed_rate
#          telemetry_type: Float
#
      - Job:
          metric: lastCheckpointDuration
          agg: max
          telemetry_path: health.last_checkpoint_duration
          telemetry_type: Integer

  # Sensors may be specified and loaded from this block. Currently, two form are supported:
  # 1. `csv` Sensors loaded at startup for the given `path` file.
  # 2. `rest_api` Sensors specified with `interval` (in seconds), HTTP `method`,`url`,
  #    `headers` (array of (String, String) tuples, and `max_retries` for each iteration.
  sensors: {}

eligibility:
  policies:
    - source: file
      policy:
        path: "./resources/eligibility.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/eligibility_ext.polar"
        is_template: true
  template_data:
    policy_extension: eligibility_ext
    cooling_secs: 900
    stable_secs: 300

decision:
  policies:
    - source: file
      policy:
        path: "./resources/decision.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/decision_basis.polar"
        is_template: true
  template_data:
    # policy template that is pulled into the base decision policy template, which forms the basis
    #of the business criteria to scale job.
    basis: decision_basis

    # Optional threshold representing the maximum ratio the total_lag rate / consumed_records_rate
    # that is healthy. This value represents how much the workload is increasing (positive) or
    # decreasing (negative). This value is used for scale up decisions and not scale down decisions.
    max_healthy_relative_lag_velocity: 3.0

    # Optional threshold representing the maximum total_lag that is healthy. This factor does not
    # consider the workload, so its usefulness may be limited compared to the
    # `max_healthy_relative_lag_velocity`. This could be used to define a cap to `total_lag` as a
    # supplemental rule to the relative lag velocity.
    # max_healthy_lag:

    # Optional threshold on the target task utilization, which is calculated based on the
    # `idle_time_millis_per_sec`. This value is used for scale down decisions and is typically
    # coupled with a check that the total_lag is 0 -- meaning the application is caught up on
    #workload and underutilized.
    min_task_utilization: 0.6

    # Optional threshold that may scale up the application if the CPU load is too high. High
    # resource utilization (CPU, Memory, Network)) may suggest the application is destabilizing,
    #which scaling up may help.
    # max_healthy_cpu_load:

    # Optional threshold that may scale up the application if the heap memory load is too high.
    # High resource utilization (CPU, Memory, Network) may suggest the application is destabilizing,
    # which scaling up may help.
    # max_healthy_heap_memory_load:

    # Optional threshold that may scale up the application if network IO is too high.
    # High resource utilization (CPU, Memory, Network) may suggest the application is destabilizing,
    #which scaling up may help.
    # max_healthy_network_io_utilization:

    # Optional threshold, among possibly others, that may indicate an application is idle. If this
    # value or min_task_utilization are not set, then rescale down for low utilization and idle
    # telemetry is disabled.
    min_idle_source_back_pressured_time_millis_per_sec: 200.0

    # Many of the rules look at the rolling average of the values to reduce the affects of
    # short-term spikes. This optional value sets the common evaluation window for these range
    # metric forms.
    # Valid duration may range from 0 (always single item) to 600 secs (10 minutes).
    evaluate_duration_secs: 120

plan:
  performance_repository:
    storage: memory

  # Optional population of node : workload rates used to estimate best cluster size for projected
  # workload. The performance history of the job is created based on the actual measured
  # performance of the job, so will change for different resourcing, configurations and even as it
  # runs. The Job's history is saved in the `performance_repository`. These default settings to tune
  # initial rescale calculations, which without initial values tend to vary wildly, but converge
  # over time while springline maps performance.

  # optional initial map of workloads at which springline will rescale upward.
  # For ech given `job_parallelism` specify the `hi_rate` (for rescale up) and/or the `lo_rate` (for
  # rescale down) default benchmark workload rates. The table can be sparse, and springline
  # calibrates its calculations accordingly.
  benchmarks:
    - job_parallelism: 1
      hi_rate    : 2.59375
    - job_parallelism: 2
      hi_rate    : 5.12963
    - job_parallelism: 3
      hi_rate    : 7.93958
    - job_parallelism: 9
      hi_rate    : 24.55
    - job_parallelism: 11
      hi_rate    : 30.4083
    - job_parallelism: 17
      hi_rate    : 47.58148
    - job_parallelism: 19
      hi_rate    : 51.50185
    - job_parallelism: 25
      hi_rate    : 70.40625
    - job_parallelism: 27
      hi_rate    : 79.3875
    - job_parallelism: 32
      hi_rate    : 90.0852
    - job_parallelism: 64
      lo_rate    : 96.27037

  min_cluster_size: 1
  min_scaling_step: 2

  max_catch_up_secs: 600
  recovery_valid_secs: 300

  # Optional starting point for restart planning for `up` and `down` rescaling.
  # Unspecified restart times for a direction is 3 minutes.
  direction_restart_secs:
    up: 220
    down: 600


  # Optional setting that directs how springline should handle detected telemetry clipping, during
  # which necessary telemetry is not provided by Flink possibly due to misconfiguration or
  # instability. For misconfiguration, if the non-source parallelism is allowed to get too high, the
  # job operators can be "overwhelmed" by the effort to aggregate data that source telemetry is not
  # published by Flink.
  # The default is to `ignore`, and the corresponding decision rules may not be able to recognize the
  # need to rescale.
  # `permanent_limit` will identify the parallelism level that clipping happens so that planning can
  # take that into account to cap the parallelism just below it.
  # `temporary_limit` allows you to set a `reset_timeout_secs` when the clipping level is reset.
  clipping_handling: ignore

  window: 20
  spike:
    std_deviation_threshold: 5.0
    influence: 0.75
    length_threshold: 3

governance:
  policy:
    policies:
      - source: file
        policy:
          path: "./resources/governance.polar"
  rules:
    min_parallelism: 1
    max_parallelism: 20
    min_cluster_size: 1
    max_cluster_size: 20
    min_scaling_step: 2
    max_scaling_step: 4
    custom: {}

action:
  # Timeout for confirmation of scaling actions. Until this timeout expires, springline will poll
  # the kubernetes API to check if the scaling action has been satisfied.
  action_timeout_secs: 600

  taskmanager:
    # A selector to identify taskmanagers the list of returned objects by the scaling target; e.g.,
    # "component=taskmanager"
    label_selector: "component=taskmanager,release=dr-springline"

    # An optional value of how much to cull taskmanagers during rescaling in order to prevent
    # taskmanager resource leaks from impacting job stability across restarts. This action is unnecessary
    # under Reactive Flink, which does not restart jobs to affect rescaling.
    # If unset (the default), taskmanager culling is disabled.
    cull_ratio: 1.0

    # Resource name of the deployment resource used to deploy taskmanagers; e.g. "statefulset/my-taskmanager".
    deploy_resource: statefulset/dr-springline-tm

    kubernetes_api:
      # Timeout for the kubernetes list/watch call.
      #
      # This limits the duration of the call, regardless of any activity or inactivity.
      # If unset for a call, the default is 290s.
      # We limit this to 295s due to [inherent watch limitations](https://github.com/kubernetes/kubernetes/issues/6513).
#      api_timeout_secs: 290

      # Interval to query kubernetes API for the status of the scaling action.
      polling_interval_secs: 5

  flink:
    # If there is one and only one uploaded jar file, optionally override entry-class on job restart.
#    entry_class: fully.qualified.class.name

    # Interval in which Flink asynchronous API status is polled. Defaults to 3 second.
    polling_interval_secs: 3

#    savepoint:
      # Time allowed for a savepoint to complete. Defaults to 10 minutes.
#      operation_timeout_secs: 600

      # Optional directory for the triggered savepoint. If not set, the default will the savepoint
      # directory configured with Flink.
#      directory: "/path/to/flink/savepoints"

    restart:
      # Time allowed restarting and confirm a FLink job. Defaults to 1 minute.
      operation_timeout_secs: 60

      # Optionally specify the restore mode when restarting from a savepoint. Defaults to "no_claim".
      # - "no_claim": Flink will not claim ownership of the snapshot files. However it will make sure
      #               it does not depend on any artifacts from the restored snapshot. In order to do
      #               that, Flink will take the first checkpoint as a full one, which means it might
      #               reupload/duplicate files that are part of the restored checkpoint.
      # - "claim": Flink will take ownership of the given snapshot. It will clean the snapshot once
      #            it is subsumed by newer ones.
      # - "legacy": This is the mode in which older version of Flink worked. It will not claim
      #             ownership of the snapshot and will not delete the files. However, it can directly
      #             depend on the existence of the files of the restored checkpoint. It might not be
      #             safe to delete checkpoints that were restored in legacy mode.
      restore_mode: claim

      # Optional boolean value that specifies whether the job submission should be rejected if the
      # savepoint contains state that cannot be mapped back to the job.
#      allow_non_restartable_jobs: false

      # Optional comma-separated list of program arguments
#      program_args:
#        - "--yourFirstParam=1"
#        -  "--yourOtherCustomArg=something"

# Properties for the springline config sensor to inject values that are known at deployment time.
# Property names and types should directly match corresponding subscription requirements as
# submission.
context_stub:
  all_sinks_healthy: true
