---
engine:
  machine_id: 1
  node_id: 1
  # Specify the span for telemetry collection into a portfolio available to eligibility and decision
  # policies. It's more transparent if this span is set to a duration above what is used in a policy
  # rule, but it is not required. If a policy rule exceeds this configured span, then coverage of
  # the portfolio must be more than half, otherwise the condition is not met.
  # The default is 10 minutes.
  telemetry_portfolio_window_secs: 600

prometheus:
  # The address the Prometheus server should bind to. Default is "9898".
  port: 9898

http:
  host: 0.0.0.0
  port: 8000

flink:
  job_manager_uri_scheme: http
#  job_manager_host: localhost
  job_manager_port: 8081
  max_retries: 3
  min_retry_interval_millis: 1000
  max_retry_interval_millis: 300000
  pool_idle_timeout_secs: 60
  pool_max_idle_per_host: 5

# configure the kubernetes client used by both sesnors and actuators.
kubernetes:
  # The Kubernetes clients supports several different configuration strategies:
  # - `infer`: Infers configuration from the environment. First attempting to load in-cluster
  # environment variables, then if that fails, trying the local kubeconfig. Infer is the common
  # setting.
  # - `local_url`: Constructs client where only the cluster_url is set and everything else is
  # set to default values.
  # - `cluster_env`: Creates configuration from the cluster's environment variables following
  # the standard API Access from a Pod and relies on you having the service account's token
  # mounted, as well as having given the service account rbac access to what you need.
  # - `kube_config`: Create configuration from the default local configu file, respecting the
  # `$KUBECONFIG` evar, but otherwise default to ~/.kube/config. You can also customize what
  # context/cluster/user you want to use here, but it will default to the current-context.
  # - `custom_kube_config`: Create configuration from a Kubeconfig struct. This bypasses the n
  # normal config parsing to obtain custom functionality.
  client: infer

  # Name of the kubernetes namespace in which to work. If not set, the default
  # kubernetes namespace is used.
#  namespace:

  # Period to allow cluster to stabilize after a patch operation. Defaults to 5 seconds.
#  patch_settle_timeout_secs: 5

sensor:
  clearinghouse:
    # Optional clearinghouse ttl configuration.
    ttl:
      # Optional setting for the time to live for each clearinghouse field. The default is 5 minutes.
      # The clearinghouse is cleared during rescaling, so this value should be set to a duration
      # reflecting a break in connectivity or the job not running.
      default_ttl_secs: 300

      # Optional overrides for specific clearinghouse fields. The key is the field name, and the
      # value is the time to live in seconds.
      #ttl_overrides: {}

      # Optional list of clearinghouse fields that never expire.
      never_expire:
        - all_sinks_healthy
        - cluster.is_rescaling
        - cluster.is_deploying
        - cluster.last_deployment

    # Optional setting for the cache access counter kept for admission and eviction. The default is
    # 1000. A good  value is to set nr_counters to be 10x the number of *unique* fields expected to
    # be kept in the clearinghouse cache when full, not necessarily "cost" related.
    nr_counters: 1000

    # Optional setting for maximum cost of the items stored in the cache. The default is 100,
    # which assumes the normal case where the per-item cost is 1.
    max_cost: 100

    # Optional setting to direct how frequent the cache is checked for eviction. The default is 5
    # seconds. A good setting should consider the frequency of data pushed into the clearinghouse.
    cleanup_interval_millis: 5000

  # The `flink` sensor is more complicated that the modular sensors that can be specified via the
  # `sensor` configuration. The `flink` sensor will query the job manager's REST API for the Job
  # scope, TaskManager scope, TaskManager admin. Then it queries for the active jobs, and for each
  # active jobs, the sensor looks for desired metrics from each task vertex. There is a default set
  # of metrics that are queried for, but the user can specify additional metrics to query for via
  # the `metric_orders` configuration.
  flink:
    metrics_initial_delay_secs: 0
    metrics_interval_secs: 15

    # User can add metrics to be collected from Flink beyond the default set. Each metric is defined
    # as an "order" of the form:
    metric_orders:
      - scope: Jobs
        metric: lastCheckpointDuration
        agg: max
        telemetry_path: health.last_checkpoint_duration
        telemetry_type: Float

  # Sensors may be specified and loaded from this block. Currently, two form are supported:
  # 1. `csv` Sensors loaded at startup for the given `path` file.
  # 2. `rest_api` Sensors specified with `interval` (in seconds), HTTP `method`,`url`,
  #    `headers` (array of (String, String) tuples, and `max_retries` for each iteration.
  sensors: {}

eligibility:
  policies:
    - source: file
      policy:
        path: "./resources/eligibility.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/eligibility_basis.polar"
        is_template: true
  template_data:
    basis: eligibility_basis
    cooling_secs: 300
    stable_secs: 300

decision:
  policies:
    - source: file
      policy:
        path: "./resources/decision.polar"
        is_template: true
    - source: file
      policy:
        path: "./resources/decision_basis.polar"
        is_template: true
  template_data:
    basis: decision_basis
    max_healthy_lag: 133.0
    min_healthy_lag: 0.0
    max_healthy_network_io_utilization: 0.6
    max_healthy_cpu_load: 0.0016
    min_healthy_cpu_load: 0.0012

plan:
  performance_repository:
    storage: memory
  min_scaling_step: 2
  restart_secs: 120
  max_catch_up_secs: 600
  recovery_valid_secs: 300
  window: 20
  spike:
    std_deviation_threshold: 5.0
    influence: 0.75
    length_threshold: 3

governance:
  policy:
    policies:
      - source: file
        policy:
          path: "./resources/governance.polar"
  rules:
    min_cluster_size: 0
    max_cluster_size: 20
    min_scaling_step: 2
    max_scaling_step: 4
    custom: {}

action:
  # Timeout for confirmation of scaling actions. Until this timeout expires, springline will poll
  # the kubernetes API to check if the scaling action has been satisfied.
  action_timeout_secs: 600

  taskmanager:
    # A selector to identify taskmanagers the list of returned objects by the scaling target; e.g.,
    # "component=taskmanager"
    label_selector: "component=taskmanager,release=dr-springline"

    # Resource name of the deployment resource used to deploy taskmanagers; e.g. "statefulset/my-taskmanager".
    deploy_resource: statefulset/dr-springline-tm

    kubernetes_api:
      # Timeout for the kubernetes list/watch call.
      #
      # This limits the duration of the call, regardless of any activity or inactivity.
      # If unset for a call, the default is 290s.
      # We limit this to 295s due to [inherent watch limitations](https://github.com/kubernetes/kubernetes/issues/6513).
#      api_timeout_secs: 290

      # Interval to query kubernetes API for the status of the scaling action.
      polling_interval_secs: 5

  flink:
    # Interval in which Flink asynchronous API status is polled. Defaults to 1 second.
    polling_interval_secs: 1

#    savepoint:
      # Time allowed for a savepoint to complete. Defaults to 10 minutes.
#      operation_timeout_secs: 600

      # Optional directory for the triggered savepoint. If not set, the default will the savepoint
      # directory configured with Flink.
#      directory: "/path/to/flink/savepoints"

#    restart:
      # Time allowed restarting and confirm a FLink job. Defaults to 1 minute.
#      operation_timeout_secs: 60

      # Optional boolean value that specifies whether the job submission should be rejected if the
      # savepoint contains state that cannot be mapped back to the job.
#      allow_non_restartable_jobs: false

      # Optional comma-separated list of program arguments
#      program_args:
#        - "--yourFirstParam=1"
#        -  "--yourOtherCustomArg=something"

# Properties for the springline config sensor to inject values that are known at deployment time.
# Property names and types should directly match corresponding subscription requirements as
# submission.
context_stub:
  all_sinks_healthy: true
